{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3\n",
    "\n",
    "In this lab, we will cover 3D geometry and camera projections. Complete all work inside this notebook file and submit only this file.\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "In this first part, we will implement basic rasterization of a point cloud. Rasterization is the process of turning a 3D representation into a 2D image.\n",
    "\n",
    "The process of rasterization can be defined by the folowing equation. \n",
    "\n",
    "![](image.png \"Projection Matrix\")\n",
    "\n",
    "The projection matrix is used to convert from 3D read world coordinates to 2D image coordinates and is formed through the multiplication of the camera intrinsic matrix, which holds properties about the camera such as focal length and near and far plane, and the camera extrinsic matrix, which holds geometric properties of the camera such as rotation and translation.  \n",
    "\n",
    "### Step 1: Defining the Extrinsic Matrix\n",
    "\n",
    "The extrinsic matrix holds the transformation the points should take to transform from the world to the camera coordinate system. And can be defined as \n",
    "\n",
    "$$\n",
    "[R|t] =\n",
    "\\begin{bmatrix}\n",
    "r_{1,1} & r_{1,2} & r_{1,3} & t_1 \\\\\n",
    "r_{2,1} & r_{2,2} & r_{2,3} & t_2 \\\\\n",
    "r_{3,1} & r_{3,2} & r_{3,3} & t_3 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where t is the translation and r is the 3x3 rotation matrix. The bottom row of [0,0,0,1] ensures that the extrinsic matrix is in the homogonous coordinate system. A more user-friendly way to represent the extrinsic matrix is by building it from the camera pose as opposed to from how world points should transform to camera coordinates. This is relatively easy to do, we can just take the inverse of the matrix that defines the camera pose. \n",
    "\n",
    "Our camera pose can be defined as two components, ${C}$ and ${R_c}$, the camera center is world space and the rotation matrix describes the camera's orientation in world space. Therefore the transformation matrix that represents the camera pose can be defined as \n",
    "\n",
    "$$\n",
    " [R_c|C] = \\begin{bmatrix}\n",
    "R_c & C  \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the extrinsic matrix ${[R|t]}$ from ${[R_c|C]}$ can be calculated by finding the inverse as follows, where T is the transpose:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "R & t  \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}  =\n",
    "\\begin{bmatrix}\n",
    "R_c & C  \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix} ^{-1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "R_c^T & -R_c^T C  \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A more detailed guide on this process can be found here: https://ksimek.github.io/2012/08/22/extrinsic/\n",
    "\n",
    "##### Task: Provided are the camera centre ${C}$ and the rotation matrix ${R_c}$. Following the equation above calculate ${[R|t]}$ and print the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "R_c = np.array([[-1, 0, 0],\n",
    "                [0, -1, 0],\n",
    "                [0, 0, 1]]) # Camera rotation\n",
    "\n",
    "C = np.array([2.26234174, 1.62975371, 5.0])  # Camera position\n",
    "\n",
    "# Calculate the extrinsic matrix [R|t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Defining the Intrinsic matrix\n",
    "\n",
    "The intrinsic matrix is far easier to define and is parameterised by the focal length ${f}$, the principle point offset ${x}$ and the skew ${s}$.\n",
    "\n",
    "$$\n",
    "k = \n",
    "\\begin{bmatrix}\n",
    "f_x & s & x_0 \\\\\n",
    "0 & f_y & y_0 \\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Task: Provided are the values for the focal length, principle point offset and skew. Define and print the intrinsic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 800\n",
    "fy = 800\n",
    "ppx = 400\n",
    "ppy = 400\n",
    "s = 0\n",
    "\n",
    "#Solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Forming the projection matrix\n",
    "\n",
    "The projection matrix is calculated by multiplying the intrinsic matrix with the extrinsic matrix, defined as\n",
    "\n",
    "$$\n",
    "P = K [R|t]\n",
    "$$\n",
    "\n",
    "##### Task: Calculate and print the projection matrix using the previously calculated intrinsic and extrinsic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the projection matrix P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Loading our data\n",
    "\n",
    "As stated above we want to render a point cloud. To load in the point cloud we will use the python package open3D. You can read more about the structure of the PointCloud object here: https://www.open3d.org/html/python_api/open3d.geometry.PointCloud.html. *Print out the first 10 points from the point cloud and the corresponding color values.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the point cloud\n",
    "point_cloud = o3d.io.read_point_cloud(\"fragment.ply\")\n",
    "\n",
    "# Print the first 10 points\n",
    "\n",
    "# print the first 10 colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Project the point cloud\n",
    "\n",
    "In this step, we will project the point cloud onto the image plane using the projection matrix we have defined above. Apply the transformation matrix to the point cloud data and print out the first 10 projected points (the uv coordinates, see fig 1).\n",
    "\n",
    "To do this you will first need to convert the points to homogeneous coordinates like so [x,y,z,1]. You will then need to apply the projection matrix. Finally you will need to normalize the projected points by the third coordinate, like `projected_point /= projected_point[2]` to get 2D image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Get the points from the point cloud\n",
    "\n",
    "# Convert points to homogeneous coordinates\n",
    "\n",
    "# Apply the projection matrix\n",
    "\n",
    "# Normalize by the third coordinate to get 2D image coordinates\n",
    "\n",
    "\n",
    "# Print the first 10 projected points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Rasterize the points onto the image array\n",
    "\n",
    "We will rasterize the points onto an image plane of size 800 by 800. To do this we will first define an image buffer of shape 800x800x3. We will then iterate through each of the points, determine if it falls on the image plane (ie x <= 0 < image_width) and if so update the image buffer with the colour value of the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the image size\n",
    "\n",
    "\n",
    "# Create an empty image array\n",
    "\n",
    "\n",
    "# Filter out points with invalid coordinates\n",
    "\n",
    "\n",
    "# Rasterize the points onto the image array\n",
    "\n",
    "\n",
    "# Display the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define a new camera centre and rotation matrix to render a top-down view of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "#### Q1: What happens in the algorithm described above if two points are stacked on top of each other? What changes could be made to the algorithm to address these issues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: What changes would you make to the algorithm to scale the points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: What would be the effect of increasing/decreasing the fx and fy values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
