{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62365542-4e61-4fe1-9e3f-246881bec2f4",
   "metadata": {},
   "source": [
    "# Week4: Calibration，Single view metrology， Photometric image formation\n",
    "\n",
    "This week, we build on our understanding of camera models by examining calibration and single-view metrology, learning how to determine camera parameters, address lens distortions, and infer scene geometry from a single image. We then turn to photometric image formation and reflectance models, exploring how light, surface properties, and material characteristics shape the intensity values captured by a camera. These combined insights set the stage for more advanced methods like photometric stereo and refined 3D reconstruction.\n",
    "\n",
    "In this section, we will learn about:\n",
    "\n",
    "- types of distortion caused by cameras\n",
    "- how to find the intrinsic and extrinsic properties of a camera\n",
    "- how to undistort images based off these properties\n",
    "\n",
    "Ref: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
    "\n",
    "#### 1. Please complete the following coding tasks, and then proceed to answer the three questions.\n",
    "\n",
    "#### 2. Provide written answers to the reflection questions in the Markdown cells below each question.\n",
    "\n",
    "#### 3. Submit the final .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472081d-a5d9-408a-b7cf-716f08d6c21f",
   "metadata": {},
   "source": [
    "Pinhole cameras often distort images through radial and tangential distortions. Correcting these requires understanding the camera's intrinsic parameters (like focal length and optical centers) and extrinsic parameters (orientation and position). Intrinsic parameters help form a camera matrix, a 3x3 matrix crucial for correcting lens distortions. This matrix is unique to each camera and, once computed, can correct any image taken by that camera.\n",
    "\n",
    "Let get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61948fba-4b3e-430b-8a0c-35515960d100",
   "metadata": {},
   "source": [
    "For camera calibration with OpenCV, we need images of a chessboard. Calibration requires matching 3D real world points to 2D image points. The latter, found where chessboard squares meet, is straightforward to identify. Assuming the chessboard is fixed on the XY plane (Z=0), we simplify to finding just the X,Y coordinates. These can be set as (0,0), (1,0), (2,0), etc., reflecting the chessboard squares' positions, with results scaled by square size. Without knowing the exact square size, we use relative square dimensions, referring to these 3D and 2D points as object and image points, respectively.\n",
    "\n",
    "To detect chessboard patterns, use cv.findChessboardCorners(), specifying the grid size (e.g., 7x6 for this example). The function returns corner points and a boolean indicating pattern detection success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fd8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25ecd1-e570-4fda-85e9-e0a67666ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the appropriate termination criteria\n",
    "# Criteria for corner refinement (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, max_iter, epsilon)\n",
    "criteria = (... , ..., ...)\n",
    "\n",
    "# Prepare object points for a 7x9 or 6x8 chessboard pattern\n",
    "# Hint: Use np.zeros(...) and np.mgrid(...) to generate a grid of points.\n",
    "# The shape depends on your actual chessboard size.\n",
    "objp = ...\n",
    "objp[:,:2] = ...  # Generate a grid of points in (x, y)\n",
    "\n",
    "# Arrays to store object points and image points from all images\n",
    "objpoints = []  # 3D points in real-world space\n",
    "imgpoints = []  # 2D points in the image plane\n",
    "\n",
    "# Hint: Provide the correct path to your folder containing chessboard images\n",
    "# Files: './demo/camera_calibration/*.jpg'\n",
    "images = glob.glob('...')\n",
    "\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = ...\n",
    "    \n",
    "    # Attempt to find the chessboard corners\n",
    "    # Hint: use cv.findChessboardCorners with the correct pattern size, e.g. (8,6)\n",
    "    ret, corners = cv.findChessboardCorners(gray, (..., ...), None)\n",
    "\n",
    "    if ret:\n",
    "        # If corners are found, add object points and refined corner positions\n",
    "        objpoints.append(objp)\n",
    "\n",
    "        # Refine the corner positions\n",
    "        corners2 = cv.cornerSubPix(gray, corners, (..., ...), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "        # Draw the corners on the image (cv.drawChessboardCorners)\n",
    "        cv.drawChessboardCorners(img, (..., ...), corners2, ret)\n",
    "\n",
    "        # Convert the image to RGB for matplotlib display\n",
    "        img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the image with the detected corners\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f\"Chessboard Corners Detected in {fname.split('/')[-1]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# After processing, you can attempt camera calibration using cv.calibrateCamera\n",
    "# (Not shown here, but after collecting objpoints and imgpoints, you can call:\n",
    "# ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e44d0b-eff4-499d-99e0-a31206264648",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "Now that we have our object points and image points, we are ready to go for calibration. We can use the function, cv.calibrateCamera() which returns the camera matrix, distortion coefficients, rotation and translation vectors etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfa58a-618b-44a5-9e9c-3f8058548dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill you code here, by using: cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995475cf",
   "metadata": {},
   "source": [
    "## Undistortion\n",
    "Now, we can take an image and undistort it. OpenCV comes with two methods for doing this. \n",
    "\n",
    "However first, we can refine the camera matrix based on a free scaling parameter using cv.getOptimalNewCameraMatrix(). If the scaling parameter alpha=0, it returns undistorted image with minimum unwanted pixels. So it may even remove some pixels at image corners. If alpha=1, all pixels are retained with some extra black images. This function also returns an image ROI which can be used to crop the result.\n",
    "\n",
    "So, we take a new image (a.jpg in this case. That is the first image in this chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57247f-e625-4db5-9acd-09028b619d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have obtained camera matrix (mtx) and distortion coefficients (dist)\n",
    "# from a previous calibration step. The student should have mtx, dist from calibrateCamera.\n",
    "\n",
    "# Hint: Load the distorted image. Modify the path if needed.\n",
    "img = cv.imread('...')  # e.g., './demo/a.jpg'\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Hint: Use cv.getOptimalNewCameraMatrix with appropriate parameters.\n",
    "# alpha parameter can be experimented with (0.0 to 1.0).\n",
    "# This function returns a refined camera matrix and ROI.\n",
    "newcameramtx, roi = ...\n",
    "\n",
    "# Convert the image to RGB for matplotlib display\n",
    "img_rgb = ...\n",
    "\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Raw Image (Distorted)')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close('all')\n",
    "\n",
    "#######################################\n",
    "# Undistortion\n",
    "#######################################\n",
    "# Using newcameramtx and dist to undistort the image.\n",
    "# Hint: Use cv.undistort() with the original image, mtx, dist, and newcameramtx\n",
    "\n",
    "\n",
    "undistorted_rgb = ...\n",
    "\n",
    "plt.imshow(undistorted_rgb)\n",
    "plt.title('Undistorted Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e9f8c-f4cc-40cb-bfa6-6e264c3d0ec3",
   "metadata": {},
   "source": [
    "### 1. Using <strong>cv.undistort()</strong>\n",
    "This is the easiest way. Just call the function and use ROI obtained above to crop the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91343e60-092c-40d7-9ae4-11955e4c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have:\n",
    "# - img: the distorted input image\n",
    "# - mtx: the camera matrix\n",
    "# - dist: the distortion coefficients\n",
    "# - newcameramtx: the optimized camera matrix\n",
    "# - roi: the region of interest returned by cv.getOptimalNewCameraMatrix\n",
    "\n",
    "# Undistort the image using cv.undistort\n",
    "# Hint: cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "# Convert to RGB for display\n",
    "dst_rgb = ...\n",
    "\n",
    "plt.imshow(dst_rgb)\n",
    "plt.title('Undistorted Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344f7bd-ac92-4d92-ac73-29e21ed99543",
   "metadata": {},
   "source": [
    "### 2. Using <strong>remapping</strong>\n",
    "This way is a little bit more difficult. First, find a mapping function from the distorted image to the undistorted image. Then use the remap function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e749df-e8be-48fa-90cc-56c2ce05a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use cv.initUndistortRectifyMap to create the mapping\n",
    "\n",
    "# Convert to RGB for display\n",
    "dst_rgb = ...\n",
    "\n",
    "plt.imshow(dst_rgb)\n",
    "plt.title('Remapped Undistorted Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8485fe-1dfc-42af-b338-07af3994326f",
   "metadata": {},
   "source": [
    "## Re-projection Error\n",
    "Re-projection error gives a good estimation of just how exact the found parameters are. The closer the re-projection error is to zero, the more accurate the parameters we found are. Given the intrinsic, distortion, rotation and translation matrices, we must first transform the object point to image point using cv.projectPoints(). Then, we can calculate the absolute norm between what we got with our transformation and the corner finding algorithm. To find the average error, we calculate the arithmetical mean of the errors calculated for all the calibration images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dae93-1333-4fa3-bd86-b8640ca703e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = 0\n",
    "\n",
    "# Assume we have:\n",
    "# objpoints: the list of 3D points in the real world space\n",
    "# imgpoints: the list of 2D points in the image plane\n",
    "# rvecs, tvecs: rotation and translation vectors from cv.calibrateCamera\n",
    "# mtx: camera matrix\n",
    "# dist: distortion coefficients\n",
    "\n",
    "for i in range(len(objpoints)):\n",
    "    # Hint: Use cv.projectPoints to project 3D points into the 2D image plane\n",
    "\n",
    "    # Calculate the error between the detected image points (imgpoints) and the projected points (imgpoints2)\n",
    "    # Use cv.norm with cv.NORM_L2 and divide by len(imgpoints2) to get average error per point.\n",
    "    ...\n",
    "# print(\"Total re-projection error:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8fc6a",
   "metadata": {},
   "source": [
    "#### 1. What is the goal of camera calibration, and why do we need a sufficient number of correspondences between 3D points and their image projections?\n",
    "(Write your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e95c77",
   "metadata": {},
   "source": [
    "#### 2. In single-view metrology, how can perspective projection and vanishing lines help us infer scene structure from a single image?\n",
    "(Write your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a737ff",
   "metadata": {},
   "source": [
    "#### 3. How do reflectance models, such as Lambertian and specular models, affect the interpretation of image intensities in photometric image formation?\n",
    "(Write your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba73bbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
